# -*- coding: utf-8 -*-
"""MLP_multiclass_classifier(ΝΝ)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gt-CYWTdHaFDMn7a8apsfilwHdjTX4Mm

1. Προετοιμασία δεδομένων
Διαβάζει το αρχείο Multiclassification_Database.csv.
Αφαιρεί κάποιες στήλες (Timestamp, DRB.RlcDelayUl, Label) από τα χαρακτηριστικά.
Χωρίζει τα δεδομένα σε train, validation και test sets (70/30 split, με 10% του train για validation).
Κάνει κανονικοποίηση των χαρακτηριστικών με RobustScaler ή StandardScaler.

2. Ορισμός του MLP μοντέλου
Το μοντέλο έχει αρχιτεκτονική:
15 → 16 → Dropout(0.3) → 8 → 3 (3 κατηγορίες εξόδου).
Υποστηρίζει επιλογή για χρήση Batch Normalization.

3. Απώλεια (Loss Function)
Χρησιμοποιεί Focal Loss αντί για απλό CrossEntropy, για να αντιμετωπίσει ανισορροπία μεταξύ κατηγοριών.

4. Εκπαίδευση
Εκπαιδεύει το μοντέλο για έως και 1400 εποχές με early stopping (80 εποχές χωρίς βελτίωση).
Χρησιμοποιεί CosineAnnealingWarmRestarts για προγραμματισμό learning rate.
Προσθέτει Gaussian θόρυβο στα δεδομένα εισόδου για regularization.
Υποστηρίζει Exponential Moving Average (EMA) για σταθερότερη αξιολόγηση.
Εκπαιδεύει το μοντέλο με πολλαπλά seeds και κρατά το καλύτερο (με βάση macro-F1 score).

5. Αξιολόγηση
Υπολογίζει accuracy, precision, recall, F1 score στο test set.
Εκτυπώνει classification report και confusion matrix.
Σχεδιάζει ROC curves και Precision–Recall curves για κάθε κατηγορία.
Σχεδιάζει training curves για loss και accuracy.

Τεχονολίες:
1. Focal Loss
2. EMA
3. Cosine LR scheduling
4. Balanced sampling
5. Early stopping


Ιδανικό για προβλήματα με ανισορροπία κατηγοριών και μικρά datasets.
"""

# ==== MLP 15→16→Dropout(0.3)→8→3 | AdamW 1e-3 wd=1e-4 | CosineAnnealingWarmRestarts | EarlyStop on val loss ====
import numpy as np, pandas as pd, random, torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler, label_binarize
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix,
                             roc_curve, auc, precision_recall_curve, average_precision_score)
import matplotlib.pyplot as plt

# --------------------------- toggles (change and rerun) ---------------------------
USE_BALANCED_SAMPLER = True      # balanced train batches (helps minority classes)
SCALER_KIND = "robust"           # "standard" or "robust"
USE_EMA = True                   # Exponential Moving Average for eval
EMA_DECAY = 0.9995
NOISE_STD = 0.02                 # Gaussian input noise during training (0.0 disables)
USE_FOCAL_LOSS = True            # focal loss instead of CE
FOCAL_GAMMA = 2.0                # 1.5–2.5 typical
FOCAL_ALPHA = 1.0
USE_BATCHNORM = False            # keep False to match the original architecture exactly
SEEDS = [13, 17, 23, 42, 77, 101, 123]  # train several seeds; keep the best validation model

# Cosine scheduler settings
COS_T0 = 200         # first restart (epochs)
COS_TMULT = 2        # period multiplier
COS_ETA_MIN = 1e-5   # min LR

# --------------------------- reproducibility ---------------------------
def set_seed(seed: int):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

BASE_SEED = 42
set_seed(BASE_SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --------------------------- data ---------------------------
df = pd.read_csv("Multiclassification_Database.csv")

remove_cols = ['Timestamp', 'DRB.RlcDelayUl', 'Label']
feat_cols = [c for c in df.columns if c not in remove_cols]
X_all = df[feat_cols].values.astype(np.float32)   # 15 features
y_all = df['Label'].values.astype(np.int64)       # 3 classes

# Split: 70% train (then 10% of train -> val), 30% test
X_tr, X_te, y_tr, y_te = train_test_split(
    X_all, y_all, test_size=0.30, stratify=y_all, random_state=BASE_SEED
)
X_tr, X_val, y_tr, y_val = train_test_split(
    X_tr, y_tr, test_size=0.10, stratify=y_tr, random_state=BASE_SEED
)

# Scale on TRAIN only
if SCALER_KIND.lower() == "robust":
    scaler = RobustScaler().fit(X_tr)
else:
    scaler = StandardScaler().fit(X_tr)

X_tr = scaler.transform(X_tr).astype(np.float32)
X_val = scaler.transform(X_val).astype(np.float32)
X_te  = scaler.transform(X_te ).astype(np.float32)

# Tensors
X_tr_t, y_tr_t = torch.tensor(X_tr), torch.tensor(y_tr)
X_val_t, y_val_t = torch.tensor(X_val), torch.tensor(y_val)
X_te_t,  y_te_t  = torch.tensor(X_te),  torch.tensor(y_te)

# Fixed loaders (val/test)
VAL_LOADER  = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=512, shuffle=False)
TEST_LOADER = DataLoader(TensorDataset(X_te_t,  y_te_t),  batch_size=512, shuffle=False)

# --------------------------- model ---------------------------
class MLP_16_8(nn.Module):
    def __init__(self, in_dim=15, out_dim=3, p=0.3, use_bn=False):
        super().__init__()
        if use_bn:
            self.net = nn.Sequential(
                nn.Linear(in_dim, 16),
                nn.BatchNorm1d(16),
                nn.ReLU(inplace=True),
                nn.Dropout(p=p),
                nn.Linear(16, 8),
                nn.BatchNorm1d(8),
                nn.ReLU(inplace=True),
                nn.Linear(8, out_dim)   # logits
            )
        else:
            self.net = nn.Sequential(
                nn.Linear(in_dim, 16),
                nn.ReLU(inplace=True),
                nn.Dropout(p=p),
                nn.Linear(16, 8),
                nn.ReLU(inplace=True),
                nn.Linear(8, out_dim)   # logits
            )
    def forward(self, x): return self.net(x)

input_dim = X_tr.shape[1]; num_classes = len(np.unique(y_all))
assert input_dim == 15, f"Expected 15 features, got {input_dim}"

# --------------------------- losses ---------------------------
class FocalLoss(nn.Module):
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.ce = nn.CrossEntropyLoss(reduction='none')
    def forward(self, logits, targets):
        ce = self.ce(logits, targets)           # shape [N]
        pt = torch.exp(-ce)                     # pt = softmax prob of true class
        loss = self.alpha * (1 - pt) ** self.gamma * ce
        return loss.mean() if self.reduction == 'mean' else loss.sum()

def make_loss():
    if USE_FOCAL_LOSS:
        return FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)
    return nn.CrossEntropyLoss()

# --------------------------- EMA helper ---------------------------
@torch.no_grad()
def ema_update(ema_model, model, decay=0.9995):
    for p_ema, p in zip(ema_model.parameters(), model.parameters()):
        p_ema.data.mul_(decay).add_(p.data, alpha=1.0 - decay)
    for b_ema, b in zip(ema_model.buffers(), model.buffers()):
        b_ema.copy_(b)

# --------------------------- training (one seed) ---------------------------
MAX_EPOCHS = 1400
PATIENCE   = 80         # a bit longer; still early stop on val loss
MIN_DELTA  = 1e-4

def make_train_loader(seed: int):
    g = torch.Generator(); g.manual_seed(seed)
    if not USE_BALANCED_SAMPLER:
        return DataLoader(TensorDataset(X_tr_t, y_tr_t), batch_size=512, shuffle=True, generator=g)
    # Balanced sampler: inverse-frequency weights on TRAIN
    class_counts = np.bincount(y_tr)
    class_weights = 1.0 / np.maximum(class_counts, 1)
    sample_weights = class_weights[y_tr]
    sampler = WeightedRandomSampler(
        weights=torch.tensor(sample_weights, dtype=torch.double),
        num_samples=len(sample_weights),
        replacement=True,
        generator=g
    )
    return DataLoader(TensorDataset(X_tr_t, y_tr_t), batch_size=512, sampler=sampler)

def train_once(seed: int):
    set_seed(seed)
    TRAIN_LOADER = make_train_loader(seed)

    model = MLP_16_8(in_dim=input_dim, out_dim=num_classes, p=0.3, use_bn=USE_BATCHNORM).to(device)
    loss_fn  = make_loss()
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=COS_T0, T_mult=COS_TMULT, eta_min=COS_ETA_MIN
    )

    ema_model = None
    if USE_EMA:
        ema_model = MLP_16_8(in_dim=input_dim, out_dim=num_classes, p=0.3, use_bn=USE_BATCHNORM).to(device)
        ema_model.load_state_dict(model.state_dict())
        for p in ema_model.parameters(): p.requires_grad_(False)

    best_val = float("inf"); best_state = None; no_improve = 0
    tr_loss_hist, va_loss_hist, tr_acc_hist, va_acc_hist = [], [], [], []

    for epoch in range(MAX_EPOCHS):
        # Train
        model.train()
        tr_loss = 0.0; tr_preds, tr_tgts = [], []
        for xb, yb in TRAIN_LOADER:
            xb = xb.to(device).float(); yb = yb.to(device).long()
            if NOISE_STD > 0:
                xb = xb + NOISE_STD * torch.randn_like(xb)

            optimizer.zero_grad(set_to_none=True)
            logits = model(xb)
            loss = loss_fn(logits, yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step(epoch + (len(tr_preds) / max(1, len(TRAIN_LOADER))))  # per-batch update

            if USE_EMA:
                ema_update(ema_model, model, decay=EMA_DECAY)

            tr_loss += loss.item()
            tr_preds.extend(torch.argmax(logits, 1).detach().cpu().numpy())
            tr_tgts.extend(yb.detach().cpu().numpy())

        tr_loss_hist.append(tr_loss / max(1, len(TRAIN_LOADER)))
        tr_acc_hist.append(accuracy_score(tr_tgts, tr_preds))

        # Validate on current model (selection stores EMA or model)
        model.eval(); va_loss = 0.0; va_preds, va_tgts = [], []
        with torch.no_grad():
            for xb, yb in VAL_LOADER:
                xb = xb.to(device).float(); yb = yb.to(device).long()
                logits = model(xb)
                va_loss += make_loss()(logits, yb).item()  # fresh loss object (no state)
                va_preds.extend(torch.argmax(logits, 1).cpu().numpy()); va_tgts.extend(yb.cpu().numpy())
        avg_va = va_loss / max(1, len(VAL_LOADER))
        va_loss_hist.append(avg_va)
        va_acc_hist.append(accuracy_score(va_tgts, va_preds))

        # Early stopping on val loss
        if avg_va < best_val - MIN_DELTA:
            best_val = avg_va
            src = ema_model if USE_EMA else model
            best_state = {k: v.detach().cpu().clone() for k, v in src.state_dict().items()}
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= PATIENCE:
                print(f"[seed {seed}] Early stop at epoch {epoch+1} (best val {best_val:.4f})")
                break

        if (epoch + 1) % 20 == 0:
            print(f"[seed {seed}] Ep {epoch+1}/{MAX_EPOCHS} | "
                  f"TrainLoss {tr_loss_hist[-1]:.4f} | ValLoss {avg_va:.4f} | "
                  f"TrainAcc {tr_acc_hist[-1]:.4f} | ValAcc {va_acc_hist[-1]:.4f} | "
                  f"LR {optimizer.param_groups[0]['lr']:.6f}")

    # Restore best (EMA if used, else model)
    final = MLP_16_8(in_dim=input_dim, out_dim=num_classes, p=0.3, use_bn=USE_BATCHNORM).to(device)
    if best_state is not None:
        final.load_state_dict(best_state)
    else:
        final.load_state_dict((ema_model or model).state_dict())

    # Validation macro-F1 for selection
    final.eval(); val_preds, val_tgts = [], []
    with torch.no_grad():
        for xb, yb in VAL_LOADER:
            xb = xb.to(device).float()
            val_preds.extend(torch.argmax(final(xb), 1).cpu().numpy()); val_tgts.extend(yb.numpy())
    val_f1 = f1_score(val_tgts, val_preds, average="macro", zero_division=0)

    return {"seed": seed, "model": final, "best_val": best_val, "val_f1": val_f1,
            "train_loss": tr_loss_hist, "val_loss": va_loss_hist,
            "train_acc": tr_acc_hist, "val_acc": va_acc_hist}

# --------------------------- run several seeds; keep the best ---------------------------
runs = [train_once(s) for s in SEEDS]
best_run = sorted(runs, key=lambda r: (-r["val_f1"], r["best_val"]))[0]
best_model = best_run["model"].eval()
print(f"\nSelected seed: {best_run['seed']} | Val F1: {best_run['val_f1']:.4f} | Best Val Loss: {best_run['best_val']:.4f}")

# --------------------------- test evaluation (best model) ---------------------------
test_preds, test_tgts = [], []
with torch.no_grad():
    for xb, yb in TEST_LOADER:
        xb = xb.to(device).float()
        test_preds.extend(torch.argmax(best_model(xb), 1).cpu().numpy()); test_tgts.extend(yb.numpy())

acc  = accuracy_score(test_tgts, test_preds)
prec = precision_score(test_tgts, test_preds, average="macro", zero_division=0)
rec  = recall_score(test_tgts,  test_preds, average="macro", zero_division=0)
f1   = f1_score(test_tgts,      test_preds, average="macro", zero_division=0)

print("\n=== Test Metrics (best seed) ===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1:        {f1:.4f}\n")

print("Classification Report:")
print(classification_report(test_tgts, test_preds, digits=4, zero_division=0))
print("\nConfusion Matrix:")
print(confusion_matrix(test_tgts, test_preds))

# --------------------------- ROC & PR (AP) ---------------------------
class_names = [f"Class {i}" for i in range(num_classes)]  # or ["Gaming","Streaming","Browsing"]
probs_list, tgts_list = [], []
with torch.no_grad():
    for xb, yb in TEST_LOADER:
        xb = xb.to(device).float()
        probs_list.append(torch.softmax(best_model(xb), 1).cpu().numpy()); tgts_list.extend(yb.numpy().tolist())
probs = np.vstack(probs_list); tgts = np.array(tgts_list)
y_bin = label_binarize(tgts, classes=list(range(num_classes)))

plt.figure(figsize=(8,6))
for i in range(num_classes):
    fpr, tpr, _ = roc_curve(y_bin[:, i], probs[:, i]); aucv = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC={aucv:.2f})')
plt.plot([0,1],[0,1],'k--'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC Curves (best seed)'); plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(8,6))
for i in range(num_classes):
    P, R, _ = precision_recall_curve(y_bin[:, i], probs[:, i]); ap = average_precision_score(y_bin[:, i], probs[:, i])
    plt.plot(R, P, label=f'{class_names[i]} (AP={ap:.2f})')
plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision–Recall Curves (best seed)')
plt.legend(); plt.grid(True); plt.show()

# --------------------------- training curves (best seed) ---------------------------
plt.figure(figsize=(10,5))
plt.plot(best_run["train_loss"], label='Train Loss'); plt.plot(best_run["val_loss"], label='Validation Loss')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss vs. Epoch (best seed)')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(10,5))
plt.plot(best_run["train_acc"], label='Train Accuracy'); plt.plot(best_run["val_acc"], label='Validation Accuracy')
plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy vs. Epoch (best seed)')
plt.legend(); plt.grid(True); plt.show()